---
title: "Predicting World Happiness for year 2023 using Data Analysis"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "2024-04-22"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# TEAM MEMBERS

1. Guruteja Kanderi
2. Alphy Benny
3. Chandini Nalamalapu

# BUSINESS CONTEXT

Understanding the dynamics of global happiness and well-being is paramount for organizations aiming to foster environments conducive to growth and prosperity. By leveraging insights from the World Happiness Report, businesses can craft strategies that prioritize employee satisfaction, societal well-being, and sustainable development. These informed decisions not only enhance workplace culture but also contribute to the creation of resilient and thriving economies on a global scale.

# PROBLEM DESCRIPTION


In our modern, interconnected world, grasping the complex tapestry of global happiness and well-being stands as a vital endeavor. The mission of the World Happiness Report is to delve deep into the realms of human satisfaction, unraveling the diverse factors that shape it. This pursuit transcends mere academic inquiry; it represents a journey to uncover the essence of human fulfillment, guiding informed policy making and precise interventions. Through the development of a predictive framework that integrates socio-economic, health, and other relevant variables, our aim is to illuminate a path toward a world that is not only happier but also more resilient. (Please note that the analysis is based on data from the year 2023.)

# DATASET DESCRIPTION

The World Happiness Report dataset consist of several variables, such as:

1. Country : The name of the country
2. Region :  Region to which the country belongs
3. Ladder Score : the overall score of happiness in a country on an average from the collective responses of the people based on the factors impacting well being. 
4. GDP : the gross domestic product per capita in purchasing power parity (PPP) adjusted dollars.
5. Freedom : the perceived freedom to make life choices in the country.
6. Corruption : the perceived level of corruption in the government and business sectors of the country.
7. Life Expectancy : Healthy life expectancy at birth
8. Social Support : the perceived social support (ability to count on others) in the country
9. Dystopia residual : the extent to which the country's happiness score deviates from an imagined dystopian society with the least happy possible outcomes.
10. Generosity :  the perceived generosity of people in the country.

# PROJECT FLOW

# STEP 1: INCLUDING LIBRARIES:

```{r}
library(MASS)
library(tidyverse)
library(reshape2)
library(lattice)
library(ggplot2)
library(data.table)
library(DataExplorer)
library(viridis)
library(corrplot)
library(car)



```

# STEP 2: DATA PREPARATION:


# Data Loading

```{r}
OriginalData <- read.csv("C:/Users/18722/Downloads/WHR2023.csv", na.strings = c('N/A'))
```

# Checking data type

```{r}

print(class(OriginalData))

```

# Reviewing the data

```{r}
#Review the Original Data
OriginalData <- as.data.table(OriginalData)
OriginalData
```
# Finding number of observations and columns in dataset

```{r}
dim(OriginalData)
```
> There are 137 observations and 20 features in dataset.

# Reviewing the structure of data frame

```{r}
str(OriginalData)
```
> All Columns are assigned with correct data types.


# Loading required data in data frame.

```{r}
data <- subset(OriginalData, select=c("Country.name", 
                              "Regional.indicator",        
                              "Ladder.score", 
                              "Logged.GDP.per.capita", 
                              "Social.support", 
                              "Healthy.life.expectancy", 
                              "Freedom.to.make.life.choices", 
                              "Generosity", 
                              "Perceptions.of.corruption", 
                              "Dystopia...residual"))

data
```

# Renaming the names of columns for better readability

```{r}
library(dplyr)
data <- data %>% rename( Country=Country.name,
                         Region=Regional.indicator,
                         Happiness_score=Ladder.score, 
                         GDP=Logged.GDP.per.capita, 
                         Social_support=Social.support, 
                         Life_expectancy=Healthy.life.expectancy, 
                         Freedom=Freedom.to.make.life.choices,
                         Corruption=Perceptions.of.corruption, 
                         Dystopia_residual=Dystopia...residual)

data
```

# Reviewing summary of data

```{r}
summary(data)
```

# Descriptive Statistics

```{r}
Mean_Happiness_Score <- mean(data$Happiness_score)
Variance_Happiness_Score <- var(data$Happiness_score)
StdDev_Happiness_Score <- sd(data$Happiness_score)
Median_Happiness_Score <- median(data$Happiness_score)

cat("Mean Happiness score is ",Mean_Happiness_Score)
cat("\nVariance of Happiness score is ",Variance_Happiness_Score)
cat("\nStandard Deviation of Happiness score is ",StdDev_Happiness_Score)
cat("\nMedian Happiness score is ",Median_Happiness_Score)
```

# STEP 3: DATA CLEANING

# Checking missing values

```{r}
MissingValueColumnName <- colnames(data)[colSums(is.na(data)) > 0]
NumberOfMissingValues <- sum(is.na(data))
cat("There are", NumberOfMissingValues, "Missing Values in Column name", MissingValueColumnName) 
```

# Unique values in Region


```{r}
# Get the unique values of a Region column
unique_values <- data %>% distinct(Region)

# Print the unique values
print(unique_values)
```

# Combining Multiple similar named region into one

```{r}
library(dplyr)
library(forcats)
# Creating data frame for unique region
dataRegion <- data
# change region to factor
dataRegion$Region <- as.factor(dataRegion$Region)
# combine south asia and southeast asia and east asia
dataRegion <- dataRegion %>% mutate(Region = fct_recode(Region,
                             # new name         old name
                             "Southeast Asia" = "South Asia",
                             "Southeast Asia" = "East Asia"))

# Get the unique values of a Region column
unique_values <- unique(dataRegion$Region)

# Print the unique values
print(unique_values)
```

# STEP 4: CHECKING NORMAL DISTRIBUTION OF TARGET VARIABLE

# 1. Checking distribution of target variable Happiness_score using Histogram as we know that target variable is a continuous variable

```{r}
library(ggplot2)
ggplot(data=data, aes(data$Happiness_score,y = after_stat(count))) +
  geom_histogram( fill = "orange", color = "black", bins = 30)+
  theme_light(base_size = 10)+
  labs(title="Happiness score all over the world", x = "World Happiness Score", y = "Frequency")
```

> The plot suggests that the distribution is roughly bell-shaped, with a peak around the score of 6. Additionally, the plot suggests that the distribution is slightly skewed to the right.

# 2. Plot to check normal distribution of target variable

```{r}
qqnorm(data$Happiness_score, pch = 1, frame = FALSE)
qqline(data$Happiness_score, col = "red")
```

> In the above normal probability plot , the data points roughly follow a straight line which states that the data follows a normal distribution.

# STEP 5 : CHECKING OUTLIERS IN TARGET VARIABLE

# 1. Checking outliers in target variable using Boxplot

```{r}
boxplot(data$Happiness_score)
abline(h = min(data$Happiness_score), col = "Blue")
abline(h = max(data$Happiness_score), col = "Yellow")
abline(h = median(data$Happiness_score), col = "Green")
abline(h = quantile(data$Happiness_score, c(0.25, 0.75)), col = "Red")
```

>The resulting plot shows a box representing the interquartile range (IQR) of the Happiness_score variable, with whiskers extending to the minimum and maximum values within 1.5 times the IQR from the lower and upper quartiles, respectively. The horizontal reference lines are overlaid on the box plot, highlighting the minimum, maximum, median, and quartile values. By inspecting the plot, any values beyond the whiskers can be considered potential outliers. There is one data point beyond the whiskers, it should be investigated further to determine whether it is a true outliers in the data or not.

# STEP 6 : EXPLORATORY DATA ANALYSIS

# 1. Checking Top 10 Happiest Country

# 1.a. Top 10 Happiest country in the world

```{r}
# Arrange the data by Happiness_score in descending order and select the top 10 rows
Top_10_Happiest_Country <- data %>%
  arrange(desc(Happiness_score)) %>%
  slice_head(n = 10)

# Print the top 10 happiest countries
Top_10_Happiest_Country



```

# Bar plot is used to visualize the distribution of categorical variable Country with continuous variable Happiness Score for top 10 Happiest Countries in the world.

```{r}
ggplot(Top_10_Happiest_Country,aes(x=factor(Country,levels=Country),y=Happiness_score))+
  geom_bar(stat="identity",width=0.5,fill="navyblue")+
  theme(axis.text.x = element_text(angle=90, vjust=0.6))+
  theme_light(base_size = 10)+
  labs(title="Top 10 Happiest Countries",x="Country",y="Happiness Score")
```

> This plot can help us compare the happiness scores of different countries and understand which countries have the highest levels of happiness.

# 2. EDA for some predictor variables like Life Expectancy, Freedom against target variable Happiness Score.

# 2.a. Happiness Score & Life Expectency

```{r}
library(ggplot2)
library(viridis)
ggplot(data = data, aes(x = Happiness_score, y =  Life_expectancy)) + 
  geom_point(aes(color = Life_expectancy), show.legend = T, pch=1, stroke=3, size = 2) +
  theme_light(base_size = 10) + scale_color_viridis(discrete = F) +
  labs(title = "Happiness Score & Life Expectancy", fill = "Happiness Score",
       x = "Happiness Score" , y = "Life Expectancy")
```

> We are using scatter plot to investigate the distribution between 2 continuous variable Happiness score and Life expectancy where each point represents a country.This plot shows that there appears to be a pretty significant relationship between Life Expectancy and the happiness score as the data points appear to show a positive trend.

# 2.d. Happiness Score & Freedom

```{r}
ggplot(data = data, aes(x = Happiness_score, y =  Freedom)) + 
  geom_point(aes(color = Freedom), show.legend = T, pch=1, stroke=3, size = 2) +
  theme_light(base_size = 10) + scale_color_viridis(discrete = F) +
  labs(title = "Happiness Score & Freedom", fill = "Happiness Score",
       x = "Happiness Score" , y = "Freedom")
```

> As predicted this plot shows that as freedom increases so does the happiness score, but we do not know if this relationship is causal and there may be confounds.

# 3. Happiness Score & Region 

# 3.a. Regionwise Happiness score using overlay barplot

```{r}
data_means <- aggregate(dataRegion$Happiness_score, list(dataRegion$Region), mean)

data_means %>% arrange(desc(x)) %>% ggplot(aes(x = x, y = reorder(Group.1, x),  fill = Group.1)) +
  geom_bar(stat = "identity") + 
  labs(title = "Regionwise Happiness") + 
  ylab("Region") + 
  xlab("Happiness Score") + 
  scale_fill_brewer(palette = "Set3") + 
  geom_text(aes(label = x), position=position_stack(vjust=0.5),color="black",size=3)
```

> The above barplot gives the region wise Happiness Score from highest to lowest.

# 3.b. Happiness score and Region overlay histogram

```{r}
# Create the histogram using ggplot2
ggplot(dataRegion, aes(x = Happiness_score, fill = Region)) +
  geom_histogram(binwidth = 0.5, alpha = 0.5, position = "identity") +
  theme_minimal() +
  labs(title = "Overlay Histogram of Happiness Score by Region", x = "Happiness Score", y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
> We are using histogram to display the distribution of a continuous variable Happiness score broken down by a categorical variable Region. From the above boxplot, we can say that Western Europe and North America and ANZ has maximum happiness score and Sub-Saharan Africa has lowest happiness score.

# 5. Exploratory data analysis of Region with other variables.

# Combine Regions to reduce number of categories

```{r}
dataRegionCat <- dataRegion %>%
  mutate(Region = fct_recode(Region,
                            # new name         old name
                            "Africa" = "Sub-Saharan Africa",    
                            "ME"     = "Middle East and North Africa",   
                            "N AMR"  = "North America and ANZ",    
                            "CIS"    = "Commonwealth of Independent States",   
                            "S AMR"  = "Latin America and Caribbean",    
                            "SE Asia"= "Southeast Asia", 
                            "EUR"    = "Central and Eastern Europe", 
                            "EUR"    = "Western Europe" ))
#Reviewing the cropped names of Region column
dataRegionCat
```


## Create a new factor variable called score

```{r}
library(dplyr)
library(tidyr)
# Create a New Factor Variable for Happiness

mn = min(dataRegionCat$Happiness_score)
mx = max(dataRegionCat$Happiness_score)
b = (mx - mn)/3.0
brk = seq(from = (mn-0.001), to = mx, by = b)

# create new categorical variable
dataRegionCat$Score <- cut(dataRegionCat$Happiness_score, breaks = brk,labels = c("Low", "Medium", "High"))
dataRegionCat <- dataRegionCat %>% drop_na()

#Reviewing Score column in dataset
dataRegionCat
```

# 5.f. Region by Generosity

```{r}
ggplot(data = dataRegionCat, aes(x = reorder(Region, Generosity),
                     y = Generosity, fill = Score)) +
  geom_boxplot(show.legend = T) + 
  labs(x = "", fill = "Score", y = "Generosity", 
       title = "Region & Generosity") + 
  scale_fill_manual(values = c("deepskyblue","olivedrab2", "yellow")) +
  theme_light(base_size = 10)
```

>  Overall, this plot suggests that generosity varies significantly across regions and may be related to regional differences in happiness scores.

# STEP 7 : HYPOTHESIS TEST :

# The target variable Happiness score is slightly skewed as discussed above and follows nearly a normal distribution, so we will be using T-statistic instead of Z-statisitc.

# 1. Hypothesis test for one mean

# Creating a separate dataset for South East Asia.

```{r}
Southeast_Asia_Data <- subset(data, Region == "Southeast Asia")
Southeast_Asia_Data
```

# Reviewing Summary of South East Asia

```{r}
summary(Southeast_Asia_Data)
```

# Preliminary test to check one-sample t-test assumptions:

# 1. Is this a large sample? - No, because n < 30 while n = 9.

# 2. To check whether the data follow a normal distribution:

# 2.1 Shapiro-Wilk test:
# H0: NULL Hypothesis : the data are normally distributed
# H1: Alternative Hypothesis : The data is not normally distributed

```{r}
shapiro.test(Southeast_Asia_Data$Happiness_score)
```

> The output of this test will include a p-value. If the p-value is greater than 0.05, then we cannot reject the null hypothesis that the data is normally distributed. If the p-value is less than 0.05, then we can reject the null hypothesis and conclude that the data is not normally distributed. Here in South East Asia data, pvalue > 0.05 implying that the distribution of the data are not significantly different from normal distribution. In other words, we can assume the normality.

# HYPOTHESIS TEST For one mean:

# H0 : NULL Hypothesis : Average Happiness score of countires in SouthEast Asia is 5.
# H1 : Alternative Hypothesis : Average Happiness score of countires in SouthEast Asia is not equal to 5.

# mu : Mean of Happiness scores of South Asia

# H0: mu = 5
# H1: mu != 5

# Manual calculations

```{r}
mu <- 5 #Population mean as per Hypothesis
xbar <- mean(Southeast_Asia_Data$Happiness_score) #Sample mean
s <- sd(Southeast_Asia_Data$Happiness_score) #Sample Standard Deviation
n <- nrow(Southeast_Asia_Data) #Sample size

stdError <- s/sqrt(n) #Standard error
df <- n-1 #Degree of freedom
ci <- 0.95 #Confidence interval
alpha <- 1-ci #Significance level
tstatistic <- (xbar - mu)/stdError #T-statistic
tvalue <- qt(1-alpha/2,df) #Critical value of t-distribution

lowerbound <- xbar - tvalue*stdError #lower bound of confidence interval
upperbound <- xbar + tvalue*stdError #upper bound of confidence interval

p_value <- 2 * (1 - pt(abs(tstatistic), df)) #p-value

cat("Sample size is:",n)
cat("\nT-Statistic value is :",tstatistic)
cat("\nDegree of freedom is :",df)
cat("\np-value is :",p_value)
cat("\nTrue mean value is :",mu)
cat("\nConfidence Interval is :",ci*100,"%")
cat("\nLowerbound for confidence interval is :",lowerbound)
cat("\nUpperbound for confidence interval is :",upperbound)
cat("\nMean of Sample is :",xbar)
```

# One sample t-test : One-sample t-test is used to compare the mean of one sample to a known standard (or theoretical/hypothetical) mean.

```{r}
t.test(Southeast_Asia_Data$Happiness_score, mu = 5, conf.level = .95)
```

> The p-value being less than 0.05 indicates that we reject the null hypothesis. Additionally, the confidence interval does not include the null hypothesis value of 5. Therefore, we reject the null hypothesis and conclude that the mean happiness score of countries in Southeast Asia is not equal to 5.


# 2. Hypothesis test for 2 mean

# Creating two independent dataset.

```{r}
Cen_East_Europe_data <- subset(data, Region == "Central and Eastern Europe")
West_Europe_data <- subset(data, Region == "Western Europe")

Europe_data = rbind(Cen_East_Europe_data, West_Europe_data)
Europe_data
```

# Reviewing Summary of Europe

```{r}
#summary statistics by groups
group_by(Europe_data, Region) %>% summarise(count = n(), 
                                            mean = mean(Happiness_score, na.rm = TRUE), 
                                            sd = sd(Happiness_score, na.rm = TRUE))
```
>Countries in Western Europe exhibit higher mean happiness scores (6.95 compared to those in Central and Eastern Europe (5.92), with slightly greater variability in happiness scores observed in Western Europe.

# 1. Are the two samples independent?
>Yes, since scores from Central and Eastern European countries and Western European countries are not related.

# 2. To check whether the data follow a normal distribution:

# 2.1 Shapiro-Wilk test:
# H0: NULL Hypothesis : the data are normally distributed
# H1: Alternative Hypothesis : The data is not normally distributed

```{r}
shapiro.test(Cen_East_Europe_data$Happiness_score)
shapiro.test(West_Europe_data$Happiness_score)
```

> In conclusion, since the p-values (p=0.1122for Central and Eastern Europe and p=0.6426 for Western Europe) are greater than the chosen significance level of 0.05, we fail to reject the null hypothesis. Thus, we can assume that the happiness score data for both regions follow a normal distribution.


# Hypothesis test For Difference in Variance :

# A hypothesis test for two independent population variances can be used to determine whether or not the data provide statistical evidence of a difference in the variance of the Happiness score between Central and Eastern European countries and Western European countries.

# H0 : NULL Hypothesis : There is no significant difference between the variance of Happiness scores of Central and Eastern European countries and Western European countries.

# H1 : Alternative Hypothesis : There is significant difference between the variance of Happiness scores of Central and Eastern European countries and Western European countries.

# (sigma1)2 : Mean of Happiness scores of Central and Eastern European countries
# (sigma2)2 : Mean of Happiness scores of Western European countries

# H0: (sigma1)2 minus (sigma2)2 = 0
# H1: (sigma1)2 minus (sigma2)2 != 0

# Manual calculations for variance

```{r}
#Sample 1
s1 <- sd(Cen_East_Europe_data$Happiness_score) #Sample Standard Deviation of Sample1
n1 <- nrow(Cen_East_Europe_data) #Sample size of Sample1
var1 <- s1^2 #Variance of sample 1

#Sample 2
s2 <- sd(West_Europe_data$Happiness_score) #Sample Standard Deviation of Sample2
n2 <- nrow(West_Europe_data) #Sample size of Sample2
var2 <- s2^2 #Variance of sample 2

stdError <- s1^2/s2^2 #F-Statistic

df1 <- n1-1 #Degree of freedom for sample 1
df2 <- n2-1 #Degree of freedom for sample 2

ci <- 0.95 #Confidence intercal
alpha <- 1-ci #Significance level

fvalue1 <- qf(1-alpha/2, df1, df2) #Critical value of F-distribution for lower bound
fvalue2 <- qf(1-alpha/2, df2, df1) #Critical value of F-distribution for upper bound

lowerbound <- stdError/fvalue1 #lower bound of confidence interval
upperbound <- stdError*fvalue2 #upper bound of confidence interval

p_value <- 2*pf(stdError,df1,df2) #p-value


cat("\nF-Statistic value is :",stdError)
cat("\nDegree of freedom for sample 1 is :",df1)
cat("\nDegree of freedom for sample 2 is :",df2)
cat("\np-value is :",p_value)
cat("\nConfidence Interval is :",ci*100,"%")
cat("\nLowerbound for confidence interval is :",lowerbound)
cat("\nUpperbound for confidence interval is :",upperbound)
cat("\nRatio of variances is :",var1/var2)
```
>The hypothesis test suggests that there is no significant difference in the variance of Happiness scores between Central and Eastern European countries and Western European countries (p > 0.05). Therefore, we fail to reject the null hypothesis, indicating similar variability in happiness scores between the two regions.

# Using F-test to compare 2 variances 

```{r}
var.test(Cen_East_Europe_data$Happiness_score,West_Europe_data$Happiness_score,alternative = "two.sided",conf.level = 0.95)
```

> The F-test yields a p-value of p = 0.9581, exceeding the significance level alpha = 0.05. Hence, we fail to reject the null hypothesis, indicating no significant difference in variances between the two datasets. Consequently, we can proceed with the classic t-test, which assumes equality of variances.


# Independent two-sample t.test with equal variance :The Independent two-samples t-test is used to compare the mean of two independent groups.

```{r}
t.test(Happiness_score ~ Region, data = Europe_data, var.equal = TRUE, conf.level = .95)
```

>The obtained p-value (4.058e-07) is much less than the significance level of 0.05, leading to the rejection of the null hypothesis. Thus, we can conclude that there is a significant difference in mean Happiness scores between Central and Eastern Europe and Western Europe.

# STEP 8 : ASSOCIATION BETWEEN VARIABLES

# Checking coorelation between numerical variable.

# Pearson Correlation Matrix

# Creating a data frame for numerical variables only

```{r}
dataNumerical <- subset(data, select=c("Happiness_score", "GDP", "Social_support", "Life_expectancy", "Freedom", "Generosity", "Corruption", "Dystopia_residual"))
```

# Checking coorelation between numerical variable.

```{r}
corData <- data.frame(cor(dataNumerical))
corData
```

# Coorelation graph

```{r}
# Coorelation graph
library(corrplot)

# Select the columns of interest from dataNumerical
selected_columns <- dataNumerical[, c("Happiness_score", "GDP", "Social_support", "Life_expectancy", "Freedom", "Generosity", "Corruption", "Dystopia_residual")]

# Compute the correlation matrix
correlation_matrix <- cor(selected_columns)

# Plot the correlation matrix
corrplot(correlation_matrix, 
         method = "color", 
         sig.level = 0.01, 
         insig = "blank", 
         addCoef.col = "black", 
         tl.srt = 45, 
         type = "upper"
)


```
>  The correlation matrix indicates the strength and direction of linear relationships between variables. Strong positive correlations are observed between Happiness Score and factors like GDP and Social Support. Conversely, Corruption shows a moderate negative correlation with Happiness Score. Overall, this analysis provides insights into potential associations among these variables, aiding further exploration and modeling efforts More accurately, it has a very slight negative effect implying that if people of a country are generous the country is slightly unhappier, score wise. 


# Highly coorelated variable

```{r}
library(lattice)
library(ggplot2)
library(caret)
#Finding highly correlated variables
highly_correlated <- findCorrelation(cor(dataNumerical), cutoff = 0.7)
cat("Highly correlated variables are:", colnames(dataNumerical)[highly_correlated])
```
> Happiness_score and GDP is highly correlated with Happiness score.

# STEP 9 : CREATING TRAINING AND TEST DATASET

# Spliting the data into training and testing data

```{r}

data1 <- subset(data, select=c("Happiness_score", "GDP", "Social_support", "Life_expectancy", "Freedom", "Generosity", "Corruption", "Dystopia_residual","Region"))

i <- sample(2, size=nrow(data1), replace=TRUE, prob=c(0.8, 0.2))
dataTraining <- data1[i==1,]
dataTest <- data1[i==2,]

cat("Total observations in Training data set is ",nrow(dataTraining))
cat("\nTotal observations in Testing data set is ",nrow(dataTest))

```

# STEP 10 : CREATING LINEAR REGRESSION MODELS

# 1. Simple Linear Regression Model

# Constructing a simple linear regression model of Happiness Score by GDP to carry out regression on the data.

```{r}
Simple_Linear_Model <- lm(Happiness_score~GDP, data=dataTraining)
Simple_Linear_Model_Summary <- summary(Simple_Linear_Model)
Simple_Linear_Model_Summary
```

> How strong is the relationship between the predictor and the response?
p-value is close to zero, thus relationship is strong

> Is the relationship between the predictor and the response positive or negative?
The coefficient is positive and hence there is a positive relationship


# Using Simple Linear Regression Model to predict Happiness Score in dataTest.

```{r}
y_Simple_Linear_Model_Pred <-predict(object = Simple_Linear_Model, newdata = dataTest)
summary(y_Simple_Linear_Model_Pred)
```

# Predicting Test Set Results for Simple Linear Regression Model

```{r}
Simple_Linear_Model_Pred_DF <- as.data.frame(cbind(Prediction = y_Simple_Linear_Model_Pred, Actual = dataTest$Happiness_score))

Simple_Linear_Model_Pred_DF
```

> We can see that the predicted interval is varying compared to the average happiness score indicating that the prediction interval is wider than the confidence interval.

# Finding RSS, R^2, MAE, MSE, RSE values for simple linear regression model.

```{r}
library(MLmetrics)


#MAE 
Simple_Linear_Model_MAE <- MAE(y_pred = y_Simple_Linear_Model_Pred, y_true = dataTest$Happiness_score)

#MSE
Simple_Linear_Model_MSE <- MSE(y_pred = y_Simple_Linear_Model_Pred, y_true = dataTest$Happiness_score)

#RSS
Simple_Linear_Model_Residual <- resid(Simple_Linear_Model)
Simple_Linear_Model_RSS <- sum(Simple_Linear_Model_Residual^2)

#$R^2$
Simple_Linear_Model_RSquare <- Simple_Linear_Model_Summary$r.squared

#RSE
Simple_Linear_Model_RSE <- Simple_Linear_Model_Summary$sigma

cat("RSS For Simple Linear Regression Model is:",Simple_Linear_Model_RSS)
cat("\nR Squared For Simple Linear Regression Model is:",Simple_Linear_Model_RSquare)
cat("\nMAE For Simple Linear Regression Model is:",Simple_Linear_Model_MAE)
cat("\nMSE For Simple Linear Regression Model is:",Simple_Linear_Model_MSE)
cat("\nRSE For Simple Linear Regression Model is:",Simple_Linear_Model_RSE)
```


>
Mean Absolute Error is the mean of summation of absolute value of actual minus predicted response.
Mean Square Error is the mean of summation of square of the actual minus predicted response value.
Residual Sum of Square is the summation of square of the actual minus predicted response value.
The minimum the above three value, the better the model.

>
Residual Standard Error is a measure of lack of fit of the model to the data. If predicted value for one or more observations is far from actual value then RSE will be large indicating a model that does not fit the data well. What constitutes a good RSE is not well defined.

>
R2 lies between zero and 1. Higher the value, the better. But the decision is practically made on the application. 


# 2. Multiple Linear Regression Model

# Constructing a multiple linear regression model of Happiness Score by all features to carry out regression on the data.

```{r}
Multiple_Regression_Model <- lm(Happiness_score~. , data=dataTraining)
Multiple_Regression_Model_Summary <- summary(Multiple_Regression_Model)
Multiple_Regression_Model_Summary
```

>
Which predictors appear to have a statistically significant relationship to the response?
All the predictors except Region appear to have a statistically signiticant relationship to the response. 

>
What does one unit increase in corruption imply?
A one unit increase in corruption implies that Happiness Score would reduce by 0.8335.


# Now we will again construct multiple linear regression without Region variable.


```{r}
Multiple_Regression_Model <- lm(Happiness_score~. -Region , data=dataTraining)
Multiple_Regression_Model_Summary <- summary(Multiple_Regression_Model)
Multiple_Regression_Model_Summary
```

# Using Multiple Linear Regression Model to predict Happiness Score in dataTest.

```{r}
y_Multiple_Regression_Model_Pred <-predict(object = Multiple_Regression_Model, newdata = dataTest)
summary(y_Multiple_Regression_Model_Pred)
```


# Predicting Test Set Results for Multiple Linear Regression Model

```{r}
Multiple_Regression_Model_Pred_DF <- as.data.frame(cbind(Prediction = y_Multiple_Regression_Model_Pred, Actual = dataTest$Happiness_score))

Multiple_Regression_Model_Pred_DF
```

> We can see that the predicted interval is varying compared to the average happiness score indicating that the prediction interval is wider than the confidence interval.


# Finding RSS, R^2, MAE and MSE values for Multiple linear regression model.

```{r}
library(MLmetrics)

#MAE
Multiple_Regression_Model_MAE <- MAE(y_pred = y_Multiple_Regression_Model_Pred, y_true = dataTest$Happiness_score)

#MSE
Multiple_Regression_Model_MSE <- MSE(y_pred = y_Multiple_Regression_Model_Pred, y_true = dataTest$Happiness_score)

#RSS
Multiple_Regression_Model_Residual <- resid(Multiple_Regression_Model)
Multiple_Regression_Model_RSS <- sum(Multiple_Regression_Model_Residual^2)

#$R^2$
Multiple_Regression_Model_RSquare <- Multiple_Regression_Model_Summary$r.squared

#RSE
Multiple_Regression_Model_RSE <- Multiple_Regression_Model_Summary$sigma


cat("RSS For Multiple Linear Regression Model is:",Multiple_Regression_Model_RSS)
cat("\nR Squared For Multiple Linear Regression Model is:",Multiple_Regression_Model_RSquare)
cat("\nMAE For Multiple Linear Regression Model is:",Multiple_Regression_Model_MAE)
cat("\nMSE For Multiple Linear Regression Model is:",Multiple_Regression_Model_MSE)
cat("\nRSE For Multiple Linear Regression Model is:",Multiple_Regression_Model_RSE)
```
> The MSE, MAE, RSE are too small. Suggesting a good model or an overfit. Since they were calculated on testing data, we do not think it is overfitting and conclude it is a good model.


# 3. Forward Stepwise Subset Selection Linear Regression Model

> 
Begins with Null Model
Fit p simple linear regressions and add to the null model the variable that results in the lowest RSS.
Add to that model the variable that results in the lowest RSS amongst all two-variable models.
Continue until some stopping rule is satisfied, for example when all remaining variables have a p-value above some threshold.


```{r}
library(MASS)
# Create a null model 
forward_intercept_only <- lm(Happiness_score ~ 1, data=dataTraining)
# Create a full model
forward_all <- lm(Happiness_score~., data=dataTraining)
# perform forward step-wise regression
Forward_Regression_Model <- stepAIC (forward_intercept_only, direction='forward',scope = formula(forward_all))
```

# Viewing Results of Forward Stepwise Subset Selection Linear Regression Model

```{r}
# view results of forward stepwise regression
Forward_Regression_Model$anova
```

# Viewing summary for Forward Stepwise Subset Selection Linear Regression Model

```{r}
# view final model
Forward_Regression_Model_Summary <- summary(Forward_Regression_Model)
Forward_Regression_Model_Summary
```

# Using Forward Stepwise Subset Selection Linear Regression Model to predict Happiness Score in dataTest.

```{r}
y_Forward_Regression_Model_Pred <-predict(object = Forward_Regression_Model, newdata = dataTest)
summary(y_Forward_Regression_Model_Pred)
```


# Predicting Test Set Results for Forward Stepwise Subset Selection Linear Regression Model

```{r}
Forward_Regression_Model_Pred_DF <- as.data.frame(cbind(Prediction = y_Forward_Regression_Model_Pred, Actual = dataTest$Happiness_score))

Forward_Regression_Model_Pred_DF
```

> The prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about Happiness Score in comparison to the average Happiness score.


# Finding RSS, R^2, MAE and MSE values for Forward Stepwise Subset Selection Linear Regression Model

```{r}
library(MLmetrics)

#MAE
Forward_Regression_Model_MAE <- MAE(y_pred = y_Forward_Regression_Model_Pred, y_true = dataTest$Happiness_score)

#MSE
Forward_Regression_Model_MSE <- MSE(y_pred = y_Forward_Regression_Model_Pred, y_true = dataTest$Happiness_score)

#RSS
Forward_Regression_Model_Residual <- resid(Forward_Regression_Model)
Forward_Regression_Model_RSS <- sum(Forward_Regression_Model_Residual^2)

#$R^2$
Forward_Regression_Model_RSquare <- Forward_Regression_Model_Summary$r.squared

#RSE
Forward_Regression_Model_RSE <- Forward_Regression_Model_Summary$sigma

cat("RSS For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_RSS)
cat("\nR Squared For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_RSquare)
cat("\nMAE For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_MAE)
cat("\nMSE For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_MSE)
cat("\nRSE For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_RSE)
```


# 4. Backward Stepwise Subset Selection Linear Regression Model

>Start with all variables in the model.
Remove the variable with the largest p value  that is, the variable that is the least statistically significant.
The new (p1) variable model is fit, and the variable with the largest p-value is removed.
Continue until a stopping rule is reached.

```{r}
# Create a full model
backward_all <- lm(Happiness_score~., data=dataTraining)
# perform Backward step-wise regression
Backward_Regression_Model <- stepAIC (backward_all, direction='backward')
```

# Viewing Results of Backward Stepwise Subset Selection Linear Regression Model

```{r}
# view results of backward stepwise regression
Backward_Regression_Model$anova
```

# Viewing summary for Backward Stepwise Subset Selection Linear Regression Model

```{r}
# view final model
Backward_Regression_Model_Summary <- summary(Backward_Regression_Model)
Backward_Regression_Model_Summary
```

# Using Backward Stepwise Subset Selection Linear Regression Model to predict Happiness Score in dataTest.

```{r}
y_Backward_Regression_Model_Pred <-predict(object = Backward_Regression_Model, newdata = dataTest)
summary(y_Backward_Regression_Model_Pred)
```

# Using Backward Stepwise Subset Selection Linear Regression Model to predict Confidence Interval on Happiness Score in dataTest.

```{r}
y_Backward_Regression_Model_Pred_conf <-predict(object = Backward_Regression_Model, newdata = dataTest, interval="confidence")
summary(y_Backward_Regression_Model_Pred_conf)
```


# Predicting Test Set Results for Backward Stepwise Subset Selection Linear Regression Model

```{r}
Backward_Regression_Model_Pred_DF <- as.data.frame(cbind(Prediction = y_Backward_Regression_Model_Pred, Actual = dataTest$Happiness_score))

Backward_Regression_Model_Pred_DF
```

> The prediction interval is substantially wider than the confidence interval, reflecting the increased uncertainty about Happiness Score in comparison to the average Happiness score.

# Finding RSS, R^2, MAE and MSE values for Backward Stepwise Subset Selection Linear Regression Model

```{r}
library(MLmetrics)

#MAE
Backward_Regression_Model_MAE <- MAE(y_pred = y_Backward_Regression_Model_Pred, y_true = dataTest$Happiness_score)

#MSE
Backward_Regression_Model_MSE <- MSE(y_pred = y_Backward_Regression_Model_Pred, y_true = dataTest$Happiness_score)

#RSS
Backward_Regression_Model_Residual <- resid(Backward_Regression_Model)
Backward_Regression_Model_RSS <- sum(Backward_Regression_Model_Residual^2)

#$R^2$
Backward_Regression_Model_RSquare <- Backward_Regression_Model_Summary$r.squared

#RSE
Backward_Regression_Model_RSE <- Backward_Regression_Model_Summary$sigma


cat("RSS For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_RSS)
cat("\nR Squared For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_RSquare)
cat("\nMAE For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_MAE)
cat("\nMSE For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_MSE)
cat("\nRSE For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_RSE)
```


# STEP 11 : MODEL ASSESSMENT

# 1. Simple Linear Regression Model

```{r}
cat("RSS For Simple Linear Regression Model is:",Simple_Linear_Model_RSS)
cat("\nR Squared For Simple Linear Regression Model is:",Simple_Linear_Model_RSquare)
cat("\nMAE For Simple Linear Regression Model is:",Simple_Linear_Model_MAE)
cat("\nMSE For Simple Linear Regression Model is:",Simple_Linear_Model_MSE)
cat("\nRSE For Simple Linear Regression Model is:",Simple_Linear_Model_RSE)
```

# 2. Multiple Linear Regression Model

```{r}
cat("RSS For Multiple Linear Regression Model is:",Multiple_Regression_Model_RSS)
cat("\nR Squared For Multiple Linear Regression Model is:",Multiple_Regression_Model_RSquare)
cat("\nMAE For Multiple Linear Regression Model is:",Multiple_Regression_Model_MAE)
cat("\nMSE For Multiple Linear Regression Model is:",Multiple_Regression_Model_MSE)
cat("\nRSE For Multiple Linear Regression Model is:",Multiple_Regression_Model_RSE)
```


# 3. Forward Stepwise subset selection Linear Regression Model

```{r}
cat("RSS For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_RSS)
cat("\nR Squared For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_RSquare)
cat("\nMAE For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_MAE)
cat("\nMSE For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_MSE)
cat("\nRSE For Forward Stepwise Subset Selection Linear Regression Model is:",Forward_Regression_Model_RSE)
```


# 4. Backward Stepwise subset selection Linear Regression Model

```{r}
cat("RSS For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_RSS)
cat("\nR Squared For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_RSquare)
cat("\nMAE For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_MAE)
cat("\nMSE For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_MSE)
cat("\nRSE For Backward Stepwise Subset Selection Linear Regression Model is:",Backward_Regression_Model_RSE)
```

>
Based on the given metrics, it seems that all four models perform well, but they are designed to answer different questions.

>
The Simple Linear Regression model uses only one predictor variable, and it appears to have the lowest performance compared to the other models, with a higher RSS, lower R-squared, higher MAE, and higher MSE, higher RSE. This model is useful when we want to study the relationship between two variables and see how a change in one variable affects the other.

>
The Multiple Linear Regression model uses multiple predictor variables, and it has a lower RSS, higher R-squared, lower MAE, lower MSE and lower RSE than the Simple Linear Regression model. This model is helpful when we want to study the relationship between a response variable and multiple predictor variables and see how these predictors affect the response variable.

>
The Forward Stepwise and Backward Stepwise Subset Selection Linear Regression models both use a subset of predictor variables, and they have the same performance metrics as the Multiple Linear Regression model. These models are useful when we want to identify a subset of predictors that best explain the variability in the response variable.

>
Given that all four models have similar performance, the choice of which model to use depends on the research question and the available data. If the research question involves only one predictor variable, then the Simple Linear Regression model is appropriate. If the research question involves multiple predictors, then the Multiple Linear Regression model or one of the Subset Selection models may be more appropriate. If we are interested in identifying the most important predictors, then we should use one of the Subset Selection models
